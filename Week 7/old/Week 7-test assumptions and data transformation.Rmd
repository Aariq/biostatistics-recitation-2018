---
title: 'Recitation Week 7: assumptions, transformation, and non-parametric tests'
output:
  html_notebook: default
  word_document: default
---
### R Notebooks
 - Remember to write your answer outside of code chunks
 - You can make text **bold** or *italics* to highlight it
 - You can also print code in-line like so: `r rnorm(1)` (use the Preview or Knit button to produce an output and compare what you get with what appears in the .Rmd file)
 - If you load in data from a .csv file you made, please do `head(mydata)` to show us what the format is.  You can also do `kable(head(mydata))` to make a nicer looking table in the Word output. E.g.:
 
```{r demo kable}
library(knitr) #for the kable() function to produce prettier tables
kable(head(sleep))
```
 

### Visualizing data with ggplot2

```{r load ggplot}
library(ggplot2) # for ggplot.  See below.
library(ggformula) # extention for ggplot2. Required for stat_qqline() function
```

`ggplot2` is a package containing many functions for producing all sorts of plots and customizing those plots.  It works pretty differently from the plotting functions we've learned so far (so-called "base R").  The good news is that every type of plot in ggplot works *basically the same way*.  That's because the "gg" in `ggplot2` stands for "grammar of graphics" which is a philosophy of data visualization that has its origins in a 1939 book by Leland Wilkinson. 

For a great tutorial on ggplot2 check out the relevant sections of R for Data Science (http://r4ds.had.co.nz/data-visualisation.html).

For a super in-depth tutorial of ggplot2, check out the ggplot2 book by Hadley Wickham available online through Tisch Library.

Every plot starts with `ggplot(data)`.  "geoms" are layers that map your data onto some graphical aestetics.  You add this with `+`.  You must tell the geoms how to map your data using `aes()`. For example, `geom_line(aes(x = x.variable, y = y.variable, color = grouping.variable))` would make a line plot with `x.variable` on the x-axis, `y.variable` on the y-axis and different colored lines for each level of `grouping.variable`.

### Checking test assumptions
#### Normality

**Visually with histograms**
```{r histograms}
 #tell it which data to use
ggplot(sleep) +   
#tell it which data to map to the x-axis inside aes()
  geom_histogram(aes(x = extra), bins = 6) +  
#split the data by "group" and put plots vertically
  facet_wrap("group", dir = "v") +            
#add a custom x axis label
  xlab("Extra Hours of Sleep")                                
```
Based on these results, is the data normally distributed?

Histograms aren't the best way to check for normality because the shape depends on the number of bins

**With Q--Q plots**

```{r qqplots}
ggplot(sleep) +
#geom_qq wants to know what to map to "sample"
  geom_qq(aes(sample = extra)) +             
  stat_qqline(aes(sample = extra)) +
#this just flips the axes to make the plot match your text book with the sample on the x-axis and normal quantiles on the y-axis 
  coord_flip() +                             
  facet_wrap("group") +
#totally optional, but I personally like the black and white theme  
  theme_bw()
```
Based on these results, is the data normally distributed?



*Advanced: use `plot_grid()` from the `cowplot` package to plot the histograms and qqplots together in a grid*
```{r advanced plot, fig.height=4, fig.width=5}
library(cowplot) # package that includes plot_grid() and others for producing publication quality graphics
library(forcats) # adds functions for dealing with factors more easily (e.g. fct_recode())
library(dplyr) # for data manipulation (e.g. mutate())

#Rather than annotating the code for you, if you want to figure out what a line does, just comment it out with "#" and re-run
sleep2 <- mutate(sleep, group = fct_recode(group, `Drug 1` = "1", `Drug 2` = "2"))

A <- ggplot(sleep2) +
  geom_histogram(aes(x = extra), bins = 6) +
  facet_wrap("group", dir = "v") +
  xlab("Extra Hours of Sleep") +
  ylab("Frequency") +
  theme_bw()

B <- ggplot(sleep2) +
  geom_qq(aes(sample = extra)) +
  stat_qqline(aes(sample = extra)) +
  coord_flip() +
  facet_wrap("group", dir = "v") +
  ylab("Extra Hours of Sleep") +
  xlab("Normal Quantile") +
  theme_bw()

plot_grid(A,B)
```

**With Shapiro-Wilk Test**
The null hypothesis is that the sample came from a normally distributed population. This also needs to be done separately on each group
```{r shapiro test}
shapiro.test(sleep$extra[sleep$group == 1])
shapiro.test(sleep$extra[sleep$group == 2])
```

#### For paired t-tests
Paired t-tests assume that the *difference* between paired observations follows a normal distribution.  You'll need to do your tests of the assumptions of normality on $d$.

I'll also demonstrate this with the `sleep` data set, since it is actually a paired design
```{r duplicate sleep data}
sleep.paired <- sleep #copy to new object so I don't overwrite built-in data
sleep.paired
```
It would be easier to calculate the difference if the two groups were in separate columns, AKA "wide" format data.
```{r convert to wide format}
library(tidyr) #gives us many functions for manipulating data frams, AKA "data wrangling"

sleep.wide <- spread(sleep.paired, key = group, value = extra) #make into "wide" format
sleep.wide$diffs <- sleep.wide$`1` - sleep.wide$`2` #I have to use backticks (`) around the column names so R knows they aren't numbers.  You shouldn't have to worry about this.
sleep.wide
```
`spread()` takes the data set as the first argument, then `value = ` is the column you want to spread or split up.  You must also supply `key =` which is the column that contains the names for the new columns you'll create. After the data is "wide", you can create a new column of the differences between the two groups easily.

Here's a nice graphic representation of how `spread()` works: 

![](https://i.imgur.com/7Pr411q.png){ width=50% height=50%}

Now we do our tests of normality on the differences:
```{r tests on d}
ggplot(sleep.wide, aes(x = diffs)) +
  geom_histogram(bins = 5)

ggplot(sleep.wide, aes(sample = diffs)) +
  geom_qq() +
  stat_qqline() +
  coord_flip()

shapiro.test(sleep.wide$diffs)
```
Not normally distributed.


#### Homogeneity of Variances

**With Levene's Test**
This test is available in the `car` package.  If you haven't installed it, do so now.
Two functions: `levene.test()` and `leveneTest()`.  The first is deprecated and may go away in the near future.  Use the newer one. There are several ways you can use this function, but the easiest is using a formula, indicated with a `~`.  The null hypothesis is equal variances
```{r Levene test}
library(car)
leveneTest(extra ~ group, data = sleep)
```
Based on these results, are the variances equal?
Based on these results, are the data normally distributed?



### Data Transformation

What if your data don't meet assumptions?  One option is to transform the data. A useful function to do that is `mutate()` from the `dplyr` package.
```{r trees qqplot, fig.height=3, fig.width=3}
ggplot(trees, aes(sample = Volume))+  #you can put the aes() in ggplot() if it applies to all layers
  geom_qq() +
  stat_qqline() +
  coord_flip()

shapiro.test(trees$Volume)
```
Not normal!

```{r transformed tree data}
data(trees)
tree.data <- trees #it's not good practice to overwrite built in datasets, so before doing transformations I "copy" the trees data to a new data frame

tree.data$sqrt.v <- sqrt(tree.data$Volume)
tree.data$ln.v <- log(tree.data$Volume)
#note, log() is natural log.  To get log base 10, use log10()


ggplot(tree.data, aes(sample = sqrt.v)) +
  geom_qq() +
  stat_qqline() +
  coord_flip() +
  ggtitle("Square Root Transformed Volume")

ggplot(tree.data, aes(sample = ln.v)) +
  geom_qq() +
  stat_qqline() +
  coord_flip() +
  ggtitle("Ln Transformed Volume")

shapiro.test(tree.data$ln.v)
```

### Non Parametric Tests
There are a few non-parametric tests available in base R

**Equivalent to 2-sample t-test**

Mann-Whitney U (AKA Wilcoxon Rank Sum) is the non-parametric equivalent to a 2-sample t-test and is available with the function `wilcox.test()`
```{r wilcox rank sum}
wilcox.test(extra~group, data = sleep)
?wilcox.test #it has basically all the same arguments as t.test
```
**Equivalen to paired t-test**

The Wilcoxon signed rank test is equivalent to a paired t-test and is also available from `wilcox.test()` with the argument `paired = TRUE`
```{r wilcox signed rank}
wilcox.test(extra~group, paired = TRUE, data = sleep)
```

### Exercise (PPT slide)

With Natalie's Nectar data, we want to know if Nectar amounts differ between colony 57 and 41.  But first we need to check our assumptions!

 1. Does the Nectar data meet assumptions of normality and homogeneity of variances?
 2. What transformation improves the normality of the data.  Check the normality of your transformed data
 3. Perform a t-test on the original and transformed data.  Are there differences?
 4. Perform a non-parametric test on the un-transformed data.  Do the results differ from the t-test on the transformed data?

```{r read in nectar data}
(nectar.data <- read.csv("nectar data.csv"))
```


#### 1. Test Assumptions
Does the Nectar data meet assumptions of normality and homogeneity of variances?
```{r plot nectar data}
ggplot(nectar.data, aes(x = Nectar)) +
  geom_histogram(bins = 8) +
  facet_wrap("Colony") +
  theme_bw()

ggplot(nectar.data, aes(sample = Nectar)) +
  geom_qq() +
  stat_qqline() +
  coord_flip() +
  facet_wrap("Colony") +
  theme_bw()
```

```{r}
shapiro.test(nectar.data$Nectar[nectar.data$Colony == 41])
shapiro.test(nectar.data$Nectar[nectar.data$Colony == 57])


leveneTest(Nectar~as.factor(Colony), data = nectar.data)
```
Not normal, but meets assumption of homogeneity of variances

#### 2. Data Transformations
What transformation improves the normality of the data?  Check the normality of your transformed data
```{r mutate nectar data}
nectar.data$ln.n <- log(nectar.data$Nectar)
nectar.data$sqrt.n <- sqrt(nectar.data$Nectar)
nectar.data$arcsin.n <- asin(nectar.data$Nectar)
```

```{r plot transformed data}
ggplot(nectar.data, aes(sample = ln.n)) +
  geom_qq() +
  stat_qqline() +
  coord_flip() +
  facet_wrap("Colony") +
  ggtitle("Log Transformed") +
  theme_bw()
#Yikes, even worse!

ggplot(nectar.data, aes(sample = arcsin.n)) +
  geom_qq() +
  stat_qqline() +
  coord_flip() +
  facet_wrap("Colony") +
  ggtitle("Arcsin Transformed") +
  theme_bw()
#Not any better than original!

ggplot(nectar.data, aes(sample = sqrt.n)) +
  geom_qq() +
  stat_qqline() +
  coord_flip() +
  facet_wrap("Colony") +
  ggtitle("Square-Root Tranformed") +
  theme_bw()
#Nice! Looking normal!
```



```{r shapiro test nectar}
shapiro.test(nectar.data$sqrt.n[nectar.data$Colony == 41])
shapiro.test(nectar.data$sqrt.n[nectar.data$Colony == 57])
```
Still reject null hypothesis of normality for colony 57, but p-value is smaller and qqplot looks better

#### 3. t-Tests
Perform a t-test on the original and transformed data.  Are there differences?
```{r t-tests}
t.test(Nectar~Colony, data = nectar.data) #WRONG.  These data do not meet the assumption of normality!
t.test(sqrt.n~Colony, data = nectar.data)
```
With the untransformed data I would reject the null hypothesis, but not with the transformed data. Making sure your data meet the assumptions affects your conclusions!

#### 4. Non-parametric test
Perform a non-parametric test on the un-transformed data.  Do the results differ from the t-test on the transformed data?
```{r wiilcox test}
wilcox.test(Nectar~Colony, data = nectar.data)
```
This leads to a different conclusion.  It is important to pick a route (transformation or non-parametric test) and stick with it.  You should **not** try multiple tests and pick the one that gives you the p-value you "want".


